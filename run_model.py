#!/usr/bin/env python3
"""
PRM (Predictive Representation Model) è®­ç»ƒè„šæœ¬
æ”¯æŒå¤šä¸ªH5æ–‡ä»¶ã€åˆ†å¸ƒå¼è®­ç»ƒã€ç®€æ´æ—¥å¿—

ä½¿ç”¨æ–¹æ³•:
1. å•GPUè®­ç»ƒ:
   python train_prm.py --data_dir /path/to/h5/files --config standard

2. å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ:
   torchrun --nproc_per_node=4 train_prm.py --data_dir /path/to/h5/files --config standard --distributed

3. æŒ‡å®šGPUæ•°é‡:
   CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 train_prm.py --data_dir /path/to/h5/files --config large --distributed

è¾“å‡ºæ ¼å¼:
- è®­ç»ƒ/éªŒè¯è¿›åº¦æ¡æ˜¾ç¤ºå®æ—¶æŒ‡æ ‡
- æ¯epochç»“æŸåæ˜¾ç¤ºæ ¼å¼åŒ–çš„æ€»ç»“ä¿¡æ¯
- *BEST* æ ‡è®°è¡¨ç¤ºå½“å‰æœ€ä½³æ¨¡å‹
"""

import os
import sys
import argparse
import logging
import json
import time
import glob
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

import h5py
import torch
import torch.nn as nn
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.utils.data import Dataset, DataLoader, ConcatDataset
from torch.nn.parallel import DistributedDataParallel
from torch.utils.data.distributed import DistributedSampler
from tqdm import tqdm

# å¯¼å…¥ç”¨æˆ·æœ¬åœ°çš„ç¼–ç å™¨ï¼ˆéœ€è¦ç”¨æˆ·æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´ï¼‰
from model import ImgEncoder, ActionEncoder  # ç”¨æˆ·éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´å¯¼å…¥è·¯å¾„
# å¯¼å…¥PRMæ¨¡å‹
from model.PRM import (
    OptimizedEndToEndModel, create_optimized_model, 
    create_training_manager, TrainingManager
)

class H5SequenceDataset(Dataset):
    """ä» H5 æ–‡ä»¶åŠ è½½åºåˆ—æ•°æ®çš„æ•°æ®é›†"""
    
    def __init__(self, h5_filepath: str, normalize_obs: bool = True):
        """
        Args:
            h5_filepath: H5 æ–‡ä»¶è·¯å¾„
            normalize_obs: æ˜¯å¦å°†è§‚å¯Ÿå½’ä¸€åŒ–åˆ° [0,1] (uint8 -> float32 / 255)
        """
        self.h5_filepath = h5_filepath
        self.normalize_obs = normalize_obs
        self.sequence_keys = []
        
        # è·å–æ‰€æœ‰åºåˆ—çš„é”®å
        with h5py.File(h5_filepath, 'r') as f:
            self.sequence_keys = [key for key in f.keys() if key.startswith('sequence_')]
            # æŒ‰æ•°å­—é¡ºåºæ’åº
            self.sequence_keys.sort(key=lambda x: int(x.split('_')[1]))
        
        print(f"ä» {h5_filepath} åŠ è½½äº† {len(self.sequence_keys)} ä¸ªåºåˆ—")
    
    def __len__(self) -> int:
        return len(self.sequence_keys)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """è·å–ä¸€ä¸ªåºåˆ—"""
        sequence_key = self.sequence_keys[idx]
        
        with h5py.File(self.h5_filepath, 'r') as f:
            seq_group = f[sequence_key]
            
            # è¯»å–æ•°æ®
            observations = seq_group['observations'][:]  # (33, 4, 84, 84)
            actions = seq_group['actions'][:]            # (32,)
            rewards = seq_group['rewards'][:]            # (32,)
            terminations = seq_group['terminations'][:]   # (32,)
            truncations = seq_group['truncations'][:]     # (32,)
        
        # è½¬æ¢ä¸º torch tensors
        sample = {
            'observations': torch.from_numpy(observations),
            'actions': torch.from_numpy(actions).long(),
            'rewards': torch.from_numpy(rewards).float(),
            'terminations': torch.from_numpy(terminations),
            'truncations': torch.from_numpy(truncations),
            'sequence_idx': torch.tensor(idx)
        }
        
        # å½’ä¸€åŒ–è§‚å¯Ÿ (uint8 -> float32 / 255)
        if self.normalize_obs:
            sample['observations'] = sample['observations'].float() / 255.0
        else:
            sample['observations'] = sample['observations'].float()
        
        return sample

class MultiFileH5Dataset(Dataset):
    """å¤„ç†æ–‡ä»¶å¤¹å†…å¤šä¸ªH5æ–‡ä»¶çš„æ•°æ®é›†åŒ…è£…å™¨"""
    
    def __init__(self, data_dir: str, h5_pattern: str = "*.h5", normalize_obs: bool = True):
        """
        Args:
            data_dir: åŒ…å«H5æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„
            h5_pattern: H5æ–‡ä»¶åŒ¹é…æ¨¡å¼ (ä¾‹å¦‚: "*.h5", "train_*.h5")
            normalize_obs: æ˜¯å¦å½’ä¸€åŒ–è§‚å¯Ÿ
        """
        self.data_dir = data_dir
        self.h5_pattern = h5_pattern
        self.normalize_obs = normalize_obs
        self.datasets = []
        self.cumulative_sizes = []
        self.total_size = 0
        
        # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        if not os.path.isdir(data_dir):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {data_dir}")
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„H5æ–‡ä»¶
        search_pattern = os.path.join(data_dir, h5_pattern)
        h5_filepaths = sorted(glob.glob(search_pattern))
        
        if not h5_filepaths:
            raise FileNotFoundError(f"åœ¨æ–‡ä»¶å¤¹ {data_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é… '{h5_pattern}' çš„H5æ–‡ä»¶")
        
        print(f"åœ¨æ–‡ä»¶å¤¹ {data_dir} ä¸­æ‰¾åˆ° {len(h5_filepaths)} ä¸ªH5æ–‡ä»¶")
        
        # åŠ è½½æ¯ä¸ªH5æ–‡ä»¶
        for i, filepath in enumerate(h5_filepaths):
            try:
                dataset = H5SequenceDataset(filepath, normalize_obs)
                self.datasets.append(dataset)
                self.total_size += len(dataset)
                self.cumulative_sizes.append(self.total_size)
                
                # æ˜¾ç¤ºæ–‡ä»¶åè€Œä¸æ˜¯å®Œæ•´è·¯å¾„
                filename = os.path.basename(filepath)
                print(f"  æ–‡ä»¶ {i+1:2d}: {filename:30s} -> {len(dataset):6d} ä¸ªåºåˆ—")
                
            except Exception as e:
                filename = os.path.basename(filepath)
                print(f"é”™è¯¯: æ— æ³•åŠ è½½ {filename}: {e}")
                continue
        
        if not self.datasets:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®é›†!")
        
        print(f"æ€»å…±åŠ è½½äº† {self.total_size:,} ä¸ªåºåˆ—")
        print(f"å¹³å‡æ¯ä¸ªæ–‡ä»¶: {self.total_size / len(self.datasets):.1f} ä¸ªåºåˆ—")
    
    def __len__(self):
        return self.total_size
    
    def __getitem__(self, idx):
        if idx >= self.total_size:
            raise IndexError("Index out of range")
        
        # æ‰¾åˆ°å¯¹åº”çš„æ•°æ®é›†
        dataset_idx = 0
        for i, cum_size in enumerate(self.cumulative_sizes):
            if idx < cum_size:
                dataset_idx = i
                break
        
        # è®¡ç®—åœ¨è¯¥æ•°æ®é›†ä¸­çš„ç´¢å¼•
        if dataset_idx == 0:
            sample_idx = idx
        else:
            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]
        
        return self.datasets[dataset_idx][sample_idx]

def setup_logging(log_file: str = None, level: str = 'INFO', rank: int = 0):
    """è®¾ç½®ç®€æ´çš„æ—¥å¿—"""
    # åªåœ¨ä¸»è¿›ç¨‹è¾“å‡ºæ—¥å¿—
    if rank != 0:
        return
        
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    
    logger = logging.getLogger()
    logger.setLevel(getattr(logging, level.upper()))
    logger.handlers.clear()
    
    # æ§åˆ¶å°è¾“å‡º
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # æ–‡ä»¶è¾“å‡ºï¼ˆå¯é€‰ï¼‰
    if log_file:
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    dist.init_process_group(backend='nccl')
    local_rank = int(os.environ['LOCAL_RANK'])
    torch.cuda.set_device(local_rank)
    return local_rank

def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()

def is_main_process():
    """æ£€æŸ¥æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
    return not dist.is_initialized() or dist.get_rank() == 0

def print_main(*args, **kwargs):
    """åªåœ¨ä¸»è¿›ç¨‹æ‰“å°"""
    if is_main_process():
        print(*args, **kwargs)

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='PRMæ¨¡å‹è®­ç»ƒ')
    
    # === å¿…éœ€å‚æ•° ===
    parser.add_argument('--data_dir', type=str, required=True,
                       help='H5æ–‡ä»¶ç›®å½•')
    
    # === æ•°æ®å‚æ•° ===
    parser.add_argument('--h5_pattern', type=str, default='*.h5',
                       help='H5æ–‡ä»¶åŒ¹é…æ¨¡å¼')
    parser.add_argument('--train_ratio', type=float, default=0.8,
                       help='è®­ç»ƒé›†æ¯”ä¾‹')
    parser.add_argument('--normalize_obs', action='store_true', default=True,
                       help='å½’ä¸€åŒ–è§‚å¯Ÿ')
    
    # === æ¨¡å‹å‚æ•°ï¼ˆäº’æ–¥ç»„ï¼šè¦ä¹ˆé€‰configï¼Œè¦ä¹ˆè‡ªå®šä¹‰ï¼‰ ===
    model_group = parser.add_mutually_exclusive_group(required=True)
    model_group.add_argument('--config', type=str, 
                           choices=['lightweight', 'standard', 'large'],
                           help='ä½¿ç”¨é¢„å®šä¹‰æ¨¡å‹é…ç½®')
    model_group.add_argument('--custom_model', action='store_true',
                           help='ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹å‚æ•°')
    
    # === è‡ªå®šä¹‰æ¨¡å‹å‚æ•°ï¼ˆä»…åœ¨--custom_modelæ—¶ç”Ÿæ•ˆï¼‰ ===
    parser.add_argument('--d_model', type=int, default=512,
                       help='æ¨¡å‹ç»´åº¦ (ä»…è‡ªå®šä¹‰æ¨¡å¼)')
    parser.add_argument('--num_heads', type=int, default=8,
                       help='æ³¨æ„åŠ›å¤´æ•° (ä»…è‡ªå®šä¹‰æ¨¡å¼)')
    parser.add_argument('--num_layers', type=int, default=6,
                       help='Transformerå±‚æ•° (ä»…è‡ªå®šä¹‰æ¨¡å¼)')
    parser.add_argument('--d_ff', type=int, default=2048,
                       help='FFNç»´åº¦ (ä»…è‡ªå®šä¹‰æ¨¡å¼)')
    
    # === é€šç”¨æ¨¡å‹å‚æ•° ===
    parser.add_argument('--dropout', type=float, default=0.1,
                       help='Dropoutç‡')
    parser.add_argument('--seq_length', type=int, default=32,
                       help='åºåˆ—é•¿åº¦')
    parser.add_argument('--pos_encoding', type=str, default='learnable',
                       choices=['learnable', 'sinusoidal'],
                       help='ä½ç½®ç¼–ç ç±»å‹')
    parser.add_argument('--loss_type', type=str, default='combined',
                       choices=['mse', 'cosine', 'huber', 'combined'],
                       help='æŸå¤±å‡½æ•°')
    
    # === è®­ç»ƒå‚æ•° ===
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', type=int, default=100,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--learning_rate', type=float, default=1e-4,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-5,
                       help='æƒé‡è¡°å‡')
    parser.add_argument('--max_grad_norm', type=float, default=1.0,
                       help='æ¢¯åº¦è£å‰ª')
    parser.add_argument('--scheduler', type=str, default='cosine',
                       choices=['cosine', 'step', 'plateau', 'none'],
                       help='å­¦ä¹ ç‡è°ƒåº¦å™¨')
    
    # === è¯„ä¼°å’Œä¿å­˜ ===
    parser.add_argument('--eval_interval', type=int, default=5,
                       help='éªŒè¯é—´éš”(epochs)')
    parser.add_argument('--save_interval', type=int, default=10,
                       help='ä¿å­˜é—´éš”(epochs)')
    parser.add_argument('--output_dir', type=str, default='./outputs',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--experiment_name', type=str, default=None,
                       help='å®éªŒåç§°')
    parser.add_argument('--resume_from', type=str, default=None,
                       help='æ¢å¤è®­ç»ƒæ£€æŸ¥ç‚¹')
    
    # === åˆ†å¸ƒå¼è®­ç»ƒå‚æ•° ===
    parser.add_argument('--distributed', action='store_true',
                       help='ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ')
    parser.add_argument('--num_gpus', type=int, default=None,
                       help='ä½¿ç”¨çš„GPUæ•°é‡ (Noneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU)')
    parser.add_argument('--local_rank', type=int, default=-1,
                       help='æœ¬åœ°GPU rank (ç”±torchrunè‡ªåŠ¨è®¾ç½®)')
    
    # === ç³»ç»Ÿå‚æ•° ===
    parser.add_argument('--device', type=str, default='auto',
                       help='è®¾å¤‡ (auto/cpu/cuda)')
    parser.add_argument('--num_workers', type=int, default=4,
                       help='æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°')
    parser.add_argument('--log_interval', type=int, default=50,
                       help='æ—¥å¿—é—´éš”(batches)')
    
    return parser.parse_args()

def load_datasets(args) -> Tuple[Dataset, Dataset]:
    """åŠ è½½æ•°æ®é›†"""
    # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.isdir(args.data_dir):
        raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.data_dir}")
    
    # æŸ¥æ‰¾H5æ–‡ä»¶
    search_pattern = os.path.join(args.data_dir, args.h5_pattern)
    h5_files = sorted(glob.glob(search_pattern))
    
    if not h5_files:
        raise FileNotFoundError(f"åœ¨ {args.data_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é… '{args.h5_pattern}' çš„H5æ–‡ä»¶")
    
    print(f"åœ¨ç›®å½• {args.data_dir} ä¸­æ‰¾åˆ° {len(h5_files)} ä¸ªH5æ–‡ä»¶")
    
    # æ ¹æ®train_ratioåˆ†å‰²æ–‡ä»¶
    if args.train_ratio >= 1.0:
        # å¦‚æœtrain_ratio >= 1.0ï¼Œä½¿ç”¨æ‰€æœ‰æ–‡ä»¶ä½œä¸ºè®­ç»ƒé›†ï¼Œæœ€åä¸€ä¸ªæ–‡ä»¶åŒæ—¶ä½œä¸ºéªŒè¯é›†
        train_files = h5_files
        val_files = [h5_files[-1]]  # ä½¿ç”¨æœ€åä¸€ä¸ªæ–‡ä»¶ä½œä¸ºéªŒè¯é›†
        print(f"ä½¿ç”¨æ‰€æœ‰ {len(train_files)} ä¸ªæ–‡ä»¶ä½œä¸ºè®­ç»ƒé›†")
        print(f"ä½¿ç”¨æœ€å 1 ä¸ªæ–‡ä»¶ä½œä¸ºéªŒè¯é›†")
    else:
        # æ­£å¸¸åˆ†å‰²
        num_train = max(1, int(len(h5_files) * args.train_ratio))
        train_files = h5_files[:num_train]
        val_files = h5_files[num_train:] if num_train < len(h5_files) else [h5_files[-1]]
        print(f"è®­ç»ƒæ–‡ä»¶: {len(train_files)} ä¸ª")
        print(f"éªŒè¯æ–‡ä»¶: {len(val_files)} ä¸ª")
    
    # åˆ›å»ºæ•°æ®é›†
    print("\n=== åŠ è½½è®­ç»ƒæ•°æ®é›† ===")
    train_dataset = create_dataset_from_files(train_files, args.normalize_obs)
    
    print("\n=== åŠ è½½éªŒè¯æ•°æ®é›† ===") 
    val_dataset = create_dataset_from_files(val_files, args.normalize_obs)
    
    return train_dataset, val_dataset

def create_dataset_from_files(h5_files: List[str], normalize_obs: bool) -> Dataset:
    """ä»æ–‡ä»¶åˆ—è¡¨åˆ›å»ºæ•°æ®é›†çš„è¾…åŠ©å‡½æ•°"""
    datasets = []
    total_sequences = 0
    
    for i, filepath in enumerate(h5_files):
        if not os.path.exists(filepath):
            print(f"è­¦å‘Š: æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ {filepath}")
            continue
        
        try:
            dataset = H5SequenceDataset(filepath, normalize_obs)
            datasets.append(dataset)
            total_sequences += len(dataset)
            
            filename = os.path.basename(filepath)
            print(f"  æ–‡ä»¶ {i+1:2d}: {filename:30s} -> {len(dataset):6d} ä¸ªåºåˆ—")
            
        except Exception as e:
            filename = os.path.basename(filepath)
            print(f"é”™è¯¯: æ— æ³•åŠ è½½ {filename}: {e}")
            continue
    
    if not datasets:
        raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®é›†!")
    
    print(f"æ€»å…±åŠ è½½äº† {total_sequences:,} ä¸ªåºåˆ—")
    
    # ä½¿ç”¨ConcatDatasetåˆå¹¶å¤šä¸ªæ•°æ®é›†
    if len(datasets) == 1:
        return datasets[0]
    else:
        return ConcatDataset(datasets)

def create_model(args):
    """åˆ›å»ºæ¨¡å‹"""
    print_main("åˆ›å»ºæ¨¡å‹...")
    
    # åˆ›å»ºç¼–ç å™¨ï¼ˆä»ç”¨æˆ·æœ¬åœ°å¯¼å…¥ï¼‰
    if args.custom_model:
        # è‡ªå®šä¹‰æ¨¡å¼ï¼šä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„å‚æ•°
        img_encoder = ImageEncoder(d_model=args.d_model, dropout=args.dropout)
        action_encoder = ActionEncoder(d_model=args.d_model, dropout=args.dropout)
        
        model = OptimizedEndToEndModel(
            img_encoder=img_encoder,
            action_encoder=action_encoder,
            d_model=args.d_model,
            num_heads=args.num_heads,
            d_ff=args.d_ff,
            num_layers=args.num_layers,
            dropout=args.dropout,
            seq_length=args.seq_length,
            loss_type=args.loss_type,
            pos_encoding_type=args.pos_encoding
        )
        print_main(f"è‡ªå®šä¹‰æ¨¡å‹: d_model={args.d_model}, layers={args.num_layers}, heads={args.num_heads}")
    else:
        # é¢„å®šä¹‰é…ç½®æ¨¡å¼ï¼šä½¿ç”¨create_optimized_model
        configs = {
            "lightweight": {"d_model": 256, "num_heads": 8, "d_ff": 1024, "num_layers": 4},
            "standard": {"d_model": 512, "num_heads": 8, "d_ff": 2048, "num_layers": 6},
            "large": {"d_model": 768, "num_heads": 12, "d_ff": 3072, "num_layers": 8}
        }
        
        config_params = configs[args.config]
        d_model = config_params["d_model"]
        
        img_encoder = ImageEncoder(d_model=d_model, dropout=args.dropout)
        action_encoder = ActionEncoder(d_model=d_model, dropout=args.dropout)
        
        model = create_optimized_model(
            img_encoder=img_encoder,
            action_encoder=action_encoder,
            config=args.config,
            seq_length=args.seq_length,
            pos_encoding_type=args.pos_encoding,
            loss_type=args.loss_type
        )
        print_main(f"é¢„å®šä¹‰é…ç½®: {args.config}")
        print_main(f"  å‚æ•°: d_model={d_model}, layers={config_params['num_layers']}, heads={config_params['num_heads']}")
    
    # ç»Ÿè®¡å‚æ•°
    if is_main_process():
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"æ€»å‚æ•°: {total_params:,}, å¯è®­ç»ƒ: {trainable_params:,}")
    
    return model

def setup_distributed_training(model, args):
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ"""
    if args.distributed:
        # è·å–å½“å‰è¿›ç¨‹çš„rankå’Œworld_size
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        local_rank = int(os.environ.get('LOCAL_RANK', 0))
        
        # å°†æ¨¡å‹ç§»åŠ¨åˆ°å¯¹åº”çš„GPU
        device = torch.device(f'cuda:{local_rank}')
        model = model.to(device)
        
        # ä½¿ç”¨DistributedDataParallelåŒ…è£…æ¨¡å‹
        model = DistributedDataParallel(
            model, 
            device_ids=[local_rank],
            output_device=local_rank,
            find_unused_parameters=False
        )
        
        print_main(f"ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ: {world_size} ä¸ªGPU")
        return model, device, rank, world_size, local_rank
    else:
        # å•GPUæˆ–CPUè®­ç»ƒ
        if args.device == 'auto':
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            device = torch.device(args.device)
        
        model = model.to(device)
        print_main(f"ä½¿ç”¨è®¾å¤‡: {device}")
        return model, device, 0, 1, 0

def train_epoch_with_progress(trainer, train_loader, train_sampler, epoch, args, rank=0):
    """å¸¦è¿›åº¦æ¡çš„è®­ç»ƒepoch"""
    if train_sampler is not None:
        train_sampler.set_epoch(epoch)
    
    # åˆ›å»ºè¿›åº¦æ¡ï¼ˆåªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºï¼‰
    if is_main_process():
        pbar = tqdm(
            train_loader, 
            desc=f"Epoch {epoch:3d}/{args.epochs}",
            unit="batch",
            ncols=100,
            leave=False
        )
    else:
        pbar = train_loader
    
    trainer.model.train()
    epoch_metrics = {
        'loss': 0.0,
        'mse_loss': 0.0,
        'cos_similarity': 0.0,
        'num_batches': 0
    }
    
    for batch_idx, batch in enumerate(pbar):
        # å¤„ç†ä¸åŒçš„batchæ ¼å¼
        if isinstance(batch, dict):
            observations = batch['observations']
            actions = batch['actions']
        else:
            observations, actions = batch
            
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        observations = observations.to(trainer.device)
        actions = actions.to(trainer.device)
        
        # è®­ç»ƒä¸€ä¸ªbatch
        metrics = trainer.model.train_on_batch(
            observations=observations,
            actions=actions,
            optimizer=trainer.optimizer,
            max_grad_norm=trainer.max_grad_norm,
            return_attention=False
        )
        
        # ç´¯ç§¯æŒ‡æ ‡
        for key in epoch_metrics:
            if key in metrics and metrics[key] is not None:
                epoch_metrics[key] += metrics[key]
        epoch_metrics['num_batches'] += 1
        trainer.step_count += 1
        
        # å­¦ä¹ ç‡è°ƒåº¦
        if trainer.scheduler is not None:
            trainer.scheduler.step()
        
        # æ›´æ–°è¿›åº¦æ¡
        if is_main_process():
            current_lr = trainer.optimizer.param_groups[0]['lr']
            pbar.set_postfix({
                'Loss': f"{metrics['loss']:.4f}",
                'CosSim': f"{metrics['cos_similarity']:.3f}",
                'LR': f"{current_lr:.2e}"
            })
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    for key in epoch_metrics:
        if key != 'num_batches':
            epoch_metrics[key] /= epoch_metrics['num_batches']
    
    return epoch_metrics

def evaluate_with_progress(trainer, val_loader, epoch, args):
    """å¸¦è¿›åº¦æ¡çš„éªŒè¯"""
    if is_main_process():
        pbar = tqdm(
            val_loader,
            desc=f"Eval  {epoch:3d}/{args.epochs}",
            unit="batch", 
            ncols=100,
            leave=False
        )
    else:
        pbar = val_loader
    
    trainer.model.eval()
    eval_metrics = {
        'loss': 0.0,
        'mse_loss': 0.0,
        'cos_similarity_mean': 0.0,
        'accuracy_05': 0.0,
        'num_batches': 0
    }
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(pbar):
            # å¤„ç†ä¸åŒçš„batchæ ¼å¼
            if isinstance(batch, dict):
                observations = batch['observations']
                actions = batch['actions']
            else:
                observations, actions = batch
                
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            observations = observations.to(trainer.device)
            actions = actions.to(trainer.device)
            
            # è¯„ä¼°ä¸€ä¸ªbatch
            metrics = trainer.model.eval_on_batch(
                observations=observations,
                actions=actions,
                return_attention=False,
                compute_detailed_metrics=True
            )
            
            # ç´¯ç§¯æŒ‡æ ‡
            for key in eval_metrics:
                if key in metrics and metrics[key] is not None:
                    eval_metrics[key] += metrics[key]
            eval_metrics['num_batches'] += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            if is_main_process():
                pbar.set_postfix({
                    'Loss': f"{metrics['loss']:.4f}",
                    'CosSim': f"{metrics['cos_similarity_mean']:.3f}"
                })
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    for key in eval_metrics:
        if key != 'num_batches':
            eval_metrics[key] /= eval_metrics['num_batches']
    
    return eval_metrics

def print_epoch_summary(epoch, args, train_metrics, val_metrics, epoch_time, best_val_loss):
    """æ‰“å°epochæ€»ç»“ä¿¡æ¯"""
    if not is_main_process():
        return
    
    # æ ¼å¼åŒ–æ—¶é—´
    time_str = f"{epoch_time:5.1f}s"
    
    # è®­ç»ƒæŒ‡æ ‡
    train_loss = train_metrics['loss']
    train_cos = train_metrics['cos_similarity']
    
    if val_metrics is not None:
        # æœ‰éªŒè¯ç»“æœ
        val_loss = val_metrics['loss']
        val_cos = val_metrics['cos_similarity_mean']
        val_acc = val_metrics['accuracy_05']
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
        is_best = val_loss < best_val_loss
        best_marker = " *BEST*" if is_best else ""
        
        print(f"Epoch {epoch:3d}/{args.epochs} | "
              f"Train: L={train_loss:.4f} C={train_cos:.3f} | "
              f"Val: L={val_loss:.4f} C={val_cos:.3f} A={val_acc:.3f} | "
              f"Time: {time_str}{best_marker}")
    else:
        # ä»…è®­ç»ƒç»“æœ
        print(f"Epoch {epoch:3d}/{args.epochs} | "
              f"Train: L={train_loss:.4f} C={train_cos:.3f} | "
              f"Time: {time_str}")

def save_config(args, output_dir):
    """ä¿å­˜é…ç½®"""
    if not is_main_process():
        return
        
    config_path = os.path.join(output_dir, 'config.json')
    
    config_dict = vars(args).copy()
    config_dict['timestamp'] = datetime.now().isoformat()
    
    with open(config_path, 'w') as f:
        json.dump(config_dict, f, indent=2, default=str)
    
    print(f"é…ç½®å·²ä¿å­˜: {config_path}")

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è§£æå‚æ•°
    args = parse_args()
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
    rank, world_size, local_rank = 0, 1, 0
    if args.distributed:
        local_rank = setup_distributed()
        rank = dist.get_rank()
        world_size = dist.get_world_size()
    
    # éªŒè¯å‚æ•°
    if args.custom_model:
        print_main("ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹å‚æ•°")
    else:
        print_main(f"ä½¿ç”¨é¢„å®šä¹‰é…ç½®: {args.config}")
        if any([args.d_model != 512, args.num_heads != 8, args.num_layers != 6, args.d_ff != 2048]):
            print_main("æ³¨æ„: åœ¨é¢„å®šä¹‰é…ç½®æ¨¡å¼ä¸‹ï¼Œè‡ªå®šä¹‰æ¨¡å‹å‚æ•°å°†è¢«å¿½ç•¥")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if args.experiment_name is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        config_name = args.config if not args.custom_model else 'custom'
        args.experiment_name = f"prm_{config_name}_{timestamp}"
    
    output_dir = os.path.join(args.output_dir, args.experiment_name)
    if is_main_process():
        os.makedirs(output_dir, exist_ok=True)
    
    # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
    if args.distributed:
        dist.barrier()
    
    # è®¾ç½®æ—¥å¿—
    log_file = os.path.join(output_dir, 'train.log') if is_main_process() else None
    setup_logging(log_file, rank=rank)
    
    # æ‰“å°è®­ç»ƒä¿¡æ¯
    if is_main_process():
        print("=" * 80)
        print(f"PRM æ¨¡å‹è®­ç»ƒ")
        print("=" * 80)
        print(f"å®éªŒåç§°: {args.experiment_name}")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        if args.distributed:
            print(f"åˆ†å¸ƒå¼è®­ç»ƒ: {world_size} GPU(s)")
        else:
            print(f"å•æœºè®­ç»ƒ")
        print("=" * 80)
    
    # ä¿å­˜é…ç½®
    save_config(args, output_dir)
    
    try:
        # åŠ è½½æ•°æ®
        if is_main_process():
            print("ğŸ“ åŠ è½½æ•°æ®é›†...")
        train_dataset, val_dataset = load_datasets(args)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, train_sampler, val_sampler = create_data_loaders(
            train_dataset, val_dataset, args, rank, world_size
        )
        
        print_main(f"ğŸ“Š è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}, éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")
        
        # åˆ›å»ºæ¨¡å‹
        model = create_model(args)
        model, device, rank, world_size, local_rank = setup_distributed_training(model, args)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        scheduler_kwargs = {}
        if args.scheduler == 'cosine':
            scheduler_kwargs['T_max'] = args.epochs * len(train_loader)
        elif args.scheduler == 'step':
            scheduler_kwargs['step_size'] = args.epochs // 3
            scheduler_kwargs['gamma'] = 0.1
        
        trainer = create_training_manager(
            model=model,
            learning_rate=args.learning_rate,
            weight_decay=args.weight_decay,
            scheduler_type=args.scheduler if args.scheduler != 'none' else None,
            max_grad_norm=args.max_grad_norm,
            device=device,
            **scheduler_kwargs
        )
        
        # æ¢å¤è®­ç»ƒ
        start_epoch = 0
        best_val_loss = float('inf')
        
        if args.resume_from:
            if is_main_process():
                print(f"ğŸ“¤ ä»æ£€æŸ¥ç‚¹æ¢å¤: {args.resume_from}")
            checkpoint = trainer.load_checkpoint(args.resume_from)
            start_epoch = checkpoint.get('epoch', 0) + 1
            best_val_loss = checkpoint.get('best_metric', float('inf'))
        
        # è®­ç»ƒå¾ªç¯
        print_main("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        print_main("=" * 80)
        
        for epoch in range(start_epoch, args.epochs):
            epoch_start = time.time()
            
            # è®­ç»ƒ
            train_metrics = train_epoch_with_progress(
                trainer, train_loader, train_sampler, epoch, args, rank
            )
            
            # éªŒè¯
            val_metrics = None
            if epoch % args.eval_interval == 0 or epoch == args.epochs - 1:
                val_metrics = evaluate_with_progress(trainer, val_loader, epoch, args)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if is_main_process() and val_metrics['loss'] < best_val_loss:
                    best_val_loss = val_metrics['loss']
                    best_path = os.path.join(output_dir, 'best_model.pth')
                    trainer.save_checkpoint(best_path, epoch, best_val_loss)
            
            # å®šæœŸä¿å­˜
            if is_main_process() and (epoch % args.save_interval == 0 or epoch == args.epochs - 1):
                save_path = os.path.join(output_dir, f'checkpoint_epoch_{epoch}.pth')
                trainer.save_checkpoint(save_path, epoch, best_val_loss)
            
            # æ‰“å°epochæ€»ç»“
            epoch_time = time.time() - epoch_start
            print_epoch_summary(epoch, args, train_metrics, val_metrics, epoch_time, best_val_loss)
        
        # è®­ç»ƒå®Œæˆ
        if is_main_process():
            print("=" * 80)
            print(f"âœ… è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
            print("=" * 80)
        
    except KeyboardInterrupt:
        print_main("âš ï¸  è®­ç»ƒè¢«ä¸­æ–­")
        # ä¿å­˜ä¸­æ–­æ£€æŸ¥ç‚¹
        if is_main_process():
            interrupt_path = os.path.join(output_dir, 'checkpoint_interrupted.pth')
            if 'trainer' in locals():
                trainer.save_checkpoint(interrupt_path, epoch, best_val_loss)
                print(f"ğŸ’¾ ä¸­æ–­æ£€æŸ¥ç‚¹å·²ä¿å­˜: {interrupt_path}")
    
    except Exception as e:
        print_main(f"âŒ è®­ç»ƒå‡ºé”™: {e}")
        raise
    
    finally:
        # æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ
        if args.distributed:
            cleanup_distributed()

if __name__ == '__main__':
    # ä½¿ç”¨ç¤ºä¾‹å’Œè¯´æ˜
    if len(sys.argv) == 1:
        print("""
PRM æ¨¡å‹è®­ç»ƒè„šæœ¬

åŸºç¡€ä½¿ç”¨:
    python train_prm.py --data_dir /path/to/h5/files --config standard

åˆ†å¸ƒå¼è®­ç»ƒ (æ¨è):
    torchrun --nproc_per_node=4 train_prm.py \\
        --data_dir /path/to/h5/files \\
        --config large \\
        --distributed \\
        --batch_size 64 \\
        --epochs 200

æ¢å¤è®­ç»ƒ:
    torchrun --nproc_per_node=4 train_prm.py \\
        --data_dir /path/to/h5/files \\
        --config standard \\
        --distributed \\
        --resume_from outputs/experiment/best_model.pth

è¾“å‡ºè¯´æ˜:
- Epoch è¿›åº¦æ¡æ˜¾ç¤ºå®æ—¶è®­ç»ƒæŒ‡æ ‡
- L: Loss, C: Cosine Similarity, A: Accuracy
- *BEST* æ ‡è®°å½“å‰æœ€ä½³éªŒè¯ç»“æœ
- æ£€æŸ¥ç‚¹è‡ªåŠ¨ä¿å­˜åˆ° outputs/ ç›®å½•

å‚æ•°è¯´æ˜:
- --config: é¢„å®šä¹‰é…ç½® (lightweight/standard/large)
- --custom_model: ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹å‚æ•°
- --distributed: å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
- --resume_from: ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
        """)
        sys.exit(0)
    
    main()