# all_game_list = [
#        "atari/jamesbond/expert-v0", "atari/pitfall/expert-v0", 
#         "atari/robotank/expert-v0", "atari/alien/expert-v0", 
#         "atari/phoenix/expert-v0", "atari/choppercommand/expert-v0", 
#         "atari/centipede/expert-v0", "atari/krull/expert-v0",
#         "atari/frostbite/expert-v0", "atari/breakout/expert-v0", 
#         "atari/kungfumaster/expert-v0", "atari/demonattack/expert-v0", 
#         "atari/fishingderby/expert-v0", "atari/boxing/expert-v0", 
#         "atari/riverraid/expert-v0", "atari/kangaroo/expert-v0",
#         "atari/atlantis/expert-v0", "atari/gopher/expert-v0", 
#         "atari/amidar/expert-v0", "atari/bankheist/expert-v0", 
#         "atari/asteroids/expert-v0", "atari/videopinball/expert-v0", 
#         "atari/asterix/expert-v0", "atari/wizardofwor/expert-v0", 
#         "atari/timepilot/expert-v0", "atari/crazyclimber/expert-v0", 
#         "atari/mspacman/expert-v0", "atari/tutankham/expert-v0", 
#         "atari/skiing/expert-v0", "atari/enduro/expert-v0", 
#         "atari/zaxxon/expert-v0", "atari/pong/expert-v0", 
#         "atari/venture/expert-v0", "atari/roadrunner/expert-v0", 
#         "atari/freeway/expert-v0", "atari/battlezone/expert-v0", 
#         "atari/solaris/expert-v0", "atari/icehockey/expert-v0", 
#         "atari/yarsrevenge/expert-v0", "atari/doubledunk/expert-v0", 
#         "atari/spaceinvaders/expert-v0", "atari/beamrider/expert-v0", 
#         "atari/namethisgame/expert-v0", "atari/upndown/expert-v0", 
#         "atari/tennis/expert-v0", "atari/hero/expert-v0", 
#         "atari/qbert/expert-v0", "atari/surround/expert-v0", 
#         "atari/berzerk/expert-v0", "atari/assault/expert-v0", 
#         "atari/defender/expert-v0", "atari/bowling/expert-v0", 
#         "atari/montezumarevenge/expert-v0", "atari/stargunner/expert-v0", 
#         "atari/privateeye/expert-v0", "atari/seaquest/expert-v0", "atari/gravitar/expert-v0"]
# game = all_game_list[:20]
# print(game)
# view_saved_dataset.py
from Dataset_minari import D4RLSequentialDataset
import matplotlib.pyplot as plt
import torch
import minari

# dataset = D4RLSequentialDataset(
#     game_list=[],
#     load_from_file='../Data/worker_13.h5'
# )

# # Print statistics
# stats = dataset.get_statistics()
# print("Dataset Statistics:")
# for key, value in stats.items():
#     print(f"  {key}: {value}")

# # Create PyTorch DataLoader directly
# from torch.utils.data import DataLoader
# dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)

# # Test loading a batch
# for batch_idx, batch in enumerate(dataloader):
#     print(f"\nBatch {batch_idx}:")
#     print(f"  Observations shape: {batch['observations'].shape}")
#     print(f"  Actions shape: {batch['actions'].shape}")
#     print(f"  Rewards shape: {batch['rewards'].shape}")
#     print(f"  Terminations shape: {batch['terminations'].shape}")
#     print(f"  Truncations shape: {batch['truncations'].shape}")
#     print(f"  Game indices shape: {batch['game_idx'].shape}")
    
#     if batch_idx == 0:  # Only show first batch
#         break

import h5py
import numpy as np
import torch
from torch.utils.data import Dataset

# def inspect_h5_file(filepath):
#     """æ£€æŸ¥ h5 æ–‡ä»¶çš„å®é™…ç»“æ„"""
#     print(f"æ£€æŸ¥æ–‡ä»¶: {filepath}")
#     print("=" * 50)
    
#     with h5py.File(filepath, 'r') as f:
#         print("æ–‡ä»¶å±æ€§ (Attributes):")
#         if len(f.attrs) > 0:
#             for key, value in f.attrs.items():
#                 print(f"  {key}: {value}")
#         else:
#             print("  æ— å±æ€§")
        
#         print(f"\næ•°æ®é›†å’Œç»„:")
#         def print_item(name, obj):
#             if isinstance(obj, h5py.Dataset):
#                 print(f"  ğŸ“Š {name}: shape={obj.shape}, dtype={obj.dtype}")
#                 # å¦‚æœæ˜¯å°æ•°ç»„ï¼Œæ˜¾ç¤ºä¸€äº›å†…å®¹
#                 if obj.size < 20:
#                     print(f"      å†…å®¹: {obj[:]}")
#             elif isinstance(obj, h5py.Group):
#                 print(f"  ğŸ“ {name}: (group)")
        
#         f.visititems(print_item)
        
#         # ç‰¹åˆ«æ£€æŸ¥é¡¶å±‚çš„ keys
#         print(f"\né¡¶å±‚ keys: {list(f.keys())}")
        
#         return list(f.keys())

# class SimpleH5Dataset(Dataset):
#     """ç®€å•çš„ H5 æ–‡ä»¶æ•°æ®é›†è¯»å–å™¨"""
    
#     def __init__(self, filepath, normalize_obs=True):
#         self.filepath = filepath
#         self.normalize_obs = normalize_obs
#         self.data = {}
#         self.length = 0
        
#         self._load_data()
    
#     def _load_data(self):
#         """ç›´æ¥ä» h5 æ–‡ä»¶è¯»å–æ‰€æœ‰æ•°æ®åˆ°å†…å­˜"""
#         print(f"ä» {self.filepath} è¯»å–æ•°æ®...")
        
#         with h5py.File(self.filepath, 'r') as f:
#             # æ–¹æ³•1: å¦‚æœæ•°æ®æ˜¯æŒ‰åºåˆ—ç»„ç»‡çš„ (sequence_0, sequence_1, ...)
#             sequence_keys = [key for key in f.keys() if key.startswith('sequence_')]
            
#             if sequence_keys:
#                 print(f"å‘ç° {len(sequence_keys)} ä¸ªåºåˆ—")
#                 self._load_sequences(f, sequence_keys)
            
#             # æ–¹æ³•2: å¦‚æœæ•°æ®æ˜¯ç›´æ¥å­˜å‚¨çš„æ•°ç»„
#             elif 'observations' in f.keys() or 'states' in f.keys() or 'data' in f.keys():
#                 print("å‘ç°ç›´æ¥å­˜å‚¨çš„æ•°æ®æ•°ç»„")
#                 self._load_arrays(f)
            
#             # æ–¹æ³•3: å¦‚æœæœ‰å…¶ä»–ç»„ç»‡æ–¹å¼
#             else:
#                 print("å°è¯•è¯»å–æ‰€æœ‰å¯ç”¨æ•°æ®...")
#                 self._load_all_available(f)
    
#     def _load_sequences(self, f, sequence_keys):
#         """åŠ è½½åºåˆ—æ ¼å¼çš„æ•°æ®"""
#         all_obs = []
#         all_actions = []
#         all_rewards = []
#         all_dones = []
        
#         # æŒ‰æ•°å­—é¡ºåºæ’åº
#         sequence_keys.sort(key=lambda x: int(x.split('_')[1]))
        
#         for seq_key in sequence_keys:
#             seq_group = f[seq_key]
            
#             if 'observations' in seq_group:
#                 obs = seq_group['observations'][:]
#                 all_obs.append(obs)
            
#             if 'actions' in seq_group:
#                 actions = seq_group['actions'][:]
#                 all_actions.append(actions)
            
#             if 'rewards' in seq_group:
#                 rewards = seq_group['rewards'][:]
#                 all_rewards.append(rewards)
            
#             # æ£€æŸ¥ terminations æˆ– dones
#             if 'terminations' in seq_group:
#                 dones = seq_group['terminations'][:]
#                 all_dones.append(dones)
#             elif 'dones' in seq_group:
#                 dones = seq_group['dones'][:]
#                 all_dones.append(dones)
        
#         # åˆå¹¶æ‰€æœ‰åºåˆ—
#         if all_obs:
#             self.data['observations'] = np.concatenate(all_obs, axis=0)
#         if all_actions:
#             self.data['actions'] = np.concatenate(all_actions, axis=0)
#         if all_rewards:
#             self.data['rewards'] = np.concatenate(all_rewards, axis=0)
#         if all_dones:
#             self.data['dones'] = np.concatenate(all_dones, axis=0)
        
#         self.length = len(self.data['actions']) if 'actions' in self.data else len(self.data['observations'])
#         print(f"åŠ è½½äº† {self.length} ä¸ªæ•°æ®ç‚¹")
    
#     def _load_arrays(self, f):
#         """åŠ è½½ç›´æ¥å­˜å‚¨çš„æ•°ç»„æ•°æ®"""
#         for key in f.keys():
#             if isinstance(f[key], h5py.Dataset):
#                 self.data[key] = f[key][:]
#                 print(f"  åŠ è½½ {key}: shape={f[key].shape}")
        
#         # ç¡®å®šæ•°æ®é•¿åº¦
#         if 'observations' in self.data:
#             self.length = len(self.data['observations'])
#         elif 'states' in self.data:
#             self.length = len(self.data['states'])
#         else:
#             self.length = len(list(self.data.values())[0])
        
#         print(f"æ•°æ®é•¿åº¦: {self.length}")
    
#     def _load_all_available(self, f):
#         """å°è¯•åŠ è½½æ‰€æœ‰å¯ç”¨æ•°æ®"""
#         def load_recursive(group, prefix=""):
#             for key, item in group.items():
#                 full_key = f"{prefix}{key}" if prefix else key
                
#                 if isinstance(item, h5py.Dataset):
#                     self.data[full_key] = item[:]
#                     print(f"  åŠ è½½ {full_key}: shape={item.shape}")
#                 elif isinstance(item, h5py.Group):
#                     load_recursive(item, f"{full_key}/")
        
#         load_recursive(f)
        
#         if self.data:
#             self.length = len(list(self.data.values())[0])
#         else:
#             self.length = 0
    
#     def __len__(self):
#         return self.length
    
#     def __getitem__(self, idx):
#         """è·å–å•ä¸ªæ•°æ®ç‚¹"""
#         sample = {}
        
#         for key, data in self.data.items():
#             value = data[idx]
            
#             # è½¬æ¢ä¸º tensor
#             if key == 'observations' and self.normalize_obs:
#                 # å¦‚æœæ˜¯å›¾åƒæ•°æ®ï¼Œå½’ä¸€åŒ–åˆ° [0, 1]
#                 if value.dtype == np.uint8:
#                     value = value.astype(np.float32) / 255.0
#                 sample[key] = torch.FloatTensor(value)
#             elif 'action' in key.lower():
#                 sample[key] = torch.FloatTensor(value)
#             elif 'reward' in key.lower():
#                 sample[key] = torch.FloatTensor([value])
#             elif 'done' in key.lower() or 'termination' in key.lower():
#                 sample[key] = torch.BoolTensor([value])
#             else:
#                 # å…¶ä»–æ•°æ®å°è¯•è½¬æ¢ä¸º tensor
#                 try:
#                     sample[key] = torch.FloatTensor(value)
#                 except:
#                     sample[key] = value  # ä¿æŒåŸæ ¼å¼
        
#         return sample
    
#     def get_info(self):
#         """è·å–æ•°æ®é›†ä¿¡æ¯"""
#         info = {
#             'length': self.length,
#             'data_keys': list(self.data.keys()),
#             'shapes': {key: data.shape for key, data in self.data.items()}
#         }
#         return info

# # ä½¿ç”¨ç¤ºä¾‹
# if __name__ == "__main__":
    # filepath = 'Data/worker_13.h5'
    
    # # 1. å…ˆæ£€æŸ¥æ–‡ä»¶ç»“æ„
    # print("æ­¥éª¤1: æ£€æŸ¥æ–‡ä»¶ç»“æ„")
    # inspect_h5_file(filepath)
    
    # print("\n" + "="*50)
    # print("æ­¥éª¤2: åˆ›å»ºæ•°æ®é›†")
    
    # # 2. åˆ›å»ºç®€å•çš„æ•°æ®é›†
    # try:
    #     dataset = SimpleH5Dataset(filepath)
        
    #     print(f"\næ•°æ®é›†åˆ›å»ºæˆåŠŸ!")
    #     print(f"æ•°æ®é›†é•¿åº¦: {len(dataset)}")
    #     print(f"æ•°æ®é›†ä¿¡æ¯:")
    #     info = dataset.get_info()
    #     for key, value in info.items():
    #         print(f"  {key}: {value}")
        
    #     # 3. æµ‹è¯•è·å–ä¸€ä¸ªæ ·æœ¬
    #     if len(dataset) > 0:
    #         print(f"\næ ·æœ¬æµ‹è¯•:")
    #         sample = dataset[0]
    #         for key, value in sample.items():
    #             if isinstance(value, torch.Tensor):
    #                 print(f"  {key}: shape={value.shape}, dtype={value.dtype}")
    #             else:
    #                 print(f"  {key}: {value} (type: {type(value)})")
        
    # except Exception as e:
    #     print(f"é”™è¯¯: {e}")
    #     import traceback
    #     traceback.print_exc()

import h5py
import torch
import numpy as np
from torch.utils.data import Dataset
from typing import Dict, Any

class H5SequenceDataset(Dataset):
    """ä» H5 æ–‡ä»¶åŠ è½½åºåˆ—æ•°æ®çš„æ•°æ®é›†"""
    
    def __init__(self, h5_filepath: str, normalize_obs: bool = True):
        """
        Args:
            h5_filepath: H5 æ–‡ä»¶è·¯å¾„
            normalize_obs: æ˜¯å¦å°†è§‚å¯Ÿå½’ä¸€åŒ–åˆ° [0,1] (uint8 -> float32 / 255)
        """
        self.h5_filepath = h5_filepath
        self.normalize_obs = normalize_obs
        self.sequence_keys = []
        
        # è·å–æ‰€æœ‰åºåˆ—çš„é”®å
        with h5py.File(h5_filepath, 'r') as f:
            self.sequence_keys = [key for key in f.keys() if key.startswith('sequence_')]
            # æŒ‰æ•°å­—é¡ºåºæ’åº
            self.sequence_keys.sort(key=lambda x: int(x.split('_')[1]))
        
        print(f"ä» {h5_filepath} åŠ è½½äº† {len(self.sequence_keys)} ä¸ªåºåˆ—")
    
    def __len__(self) -> int:
        return len(self.sequence_keys)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """è·å–ä¸€ä¸ªåºåˆ—"""
        sequence_key = self.sequence_keys[idx]
        
        with h5py.File(self.h5_filepath, 'r') as f:
            seq_group = f[sequence_key]
            
            # è¯»å–æ•°æ®
            observations = seq_group['observations'][:]  # (33, 4, 84, 84)
            actions = seq_group['actions'][:]            # (32,)
            rewards = seq_group['rewards'][:]            # (32,)
            terminations = seq_group['terminations'][:]   # (32,)
            truncations = seq_group['truncations'][:]     # (32,)
        
        # è½¬æ¢ä¸º torch tensors
        sample = {
            'observations': torch.from_numpy(observations),
            'actions': torch.from_numpy(actions).long(),
            'rewards': torch.from_numpy(rewards).float(),
            'terminations': torch.from_numpy(terminations),
            'truncations': torch.from_numpy(truncations),
            'sequence_idx': torch.tensor(idx)
        }
        
        # å½’ä¸€åŒ–è§‚å¯Ÿ (uint8 -> float32 / 255)
        if self.normalize_obs:
            sample['observations'] = sample['observations'].float() / 255.0
        else:
            sample['observations'] = sample['observations'].float()
        
        return sample
    
    def get_sequence_info(self, idx: int = 0) -> Dict[str, Any]:
        """è·å–åºåˆ—ä¿¡æ¯ï¼ˆç”¨ç¬¬ä¸€ä¸ªåºåˆ—ä½œä¸ºç¤ºä¾‹ï¼‰"""
        if idx >= len(self.sequence_keys):
            idx = 0
            
        sample = self[idx]
        
        info = {
            'total_sequences': len(self.sequence_keys),
            'sequence_length': sample['actions'].shape[0],  # 32
            'observation_shape': sample['observations'].shape,  # (33, 4, 84, 84)
            'action_shape': sample['actions'].shape,  # (32,)
            'reward_shape': sample['rewards'].shape,  # (32,)
            'data_types': {
                'observations': sample['observations'].dtype,
                'actions': sample['actions'].dtype,
                'rewards': sample['rewards'].dtype,
                'terminations': sample['terminations'].dtype,
                'truncations': sample['truncations'].dtype,
            }
        }
        
        return info

def load_h5_dataset(h5_filepath: str, normalize_obs: bool = True) -> H5SequenceDataset:
    """
    ç®€å•çš„å‡½æ•°æ¥åŠ è½½ H5 åºåˆ—æ•°æ®é›†
    
    Args:
        h5_filepath: H5 æ–‡ä»¶è·¯å¾„
        normalize_obs: æ˜¯å¦å½’ä¸€åŒ–è§‚å¯Ÿåˆ° [0,1]
        
    Returns:
        H5SequenceDataset å®ä¾‹
    """
    return H5SequenceDataset(h5_filepath, normalize_obs)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ä½¿ç”¨ä½ çš„æ–‡ä»¶
    h5_file = 'Data/worker_13.h5'
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = load_h5_dataset(h5_file)
    
    # æŸ¥çœ‹æ•°æ®é›†ä¿¡æ¯
    print("æ•°æ®é›†ä¿¡æ¯:")
    info = dataset.get_sequence_info()
    for key, value in info.items():
        print(f"  {key}: {value}")
    
    # æµ‹è¯•è·å–ä¸€ä¸ªåºåˆ—
    print(f"\næµ‹è¯•åŠ è½½ç¬¬ä¸€ä¸ªåºåˆ—:")
    sample = dataset[0]
    for key, value in sample.items():
        print(f"  {key}: shape={value.shape}, dtype={value.dtype}")
    
    # åˆ›å»º DataLoader
    from torch.utils.data import DataLoader
    
    dataloader = DataLoader(
        dataset, 
        batch_size=8, 
        shuffle=True, 
        num_workers=0  # ä½¿ç”¨ 0 é¿å…å¤šè¿›ç¨‹é—®é¢˜
    )
    
    print(f"\næµ‹è¯• DataLoader:")
    for batch_idx, batch in enumerate(dataloader):
        print(f"Batch {batch_idx}:")
        for key, value in batch.items():
            print(f"  {key}: shape={value.shape}")
        if batch_idx == 0:  # åªæ˜¾ç¤ºç¬¬ä¸€ä¸ª batch
            break