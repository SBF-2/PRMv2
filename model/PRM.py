import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple, Literal, Union

class PositionalEncoding(nn.Module):
    """
    ä½ç½®ç¼–ç æ¨¡å—ï¼Œæ”¯æŒå¯å­¦ä¹ å’Œå›ºå®šä¸¤ç§æ¨¡å¼
    """
    
    def __init__(self, 
                 d_model: int, 
                 max_seq_length: int = 512, 
                 encoding_type: Literal['learnable', 'sinusoidal'] = 'learnable',
                 dropout: float = 0.1):
        super().__init__()
        
        self.d_model = d_model
        self.encoding_type = encoding_type
        self.dropout = nn.Dropout(dropout)
        
        if encoding_type == 'learnable':
            # å¯å­¦ä¹ çš„ä½ç½®ç¼–ç 
            self.positional_embedding = nn.Embedding(max_seq_length, d_model)
            # åˆå§‹åŒ–
            nn.init.normal_(self.positional_embedding.weight, std=0.02)
            
        elif encoding_type == 'sinusoidal':
            # å›ºå®šçš„æ­£å¼¦ä½ç½®ç¼–ç 
            pe = torch.zeros(max_seq_length, d_model)
            position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)
            
            div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                               (-math.log(10000.0) / d_model))
            
            pe[:, 0::2] = torch.sin(position * div_term)
            pe[:, 1::2] = torch.cos(position * div_term)
            
            self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_seq_length, d_model)
        
        else:
            raise ValueError(f"Unknown encoding_type: {encoding_type}")
    
    def forward(self, x: torch.Tensor, start_pos: int = 0) -> torch.Tensor:
        """
        Args:
            x: (batch_size, seq_length, d_model) - è¾“å…¥ç‰¹å¾
            start_pos: èµ·å§‹ä½ç½®ï¼ˆç”¨äºå¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—ï¼‰
        
        Returns:
            (batch_size, seq_length, d_model) - æ·»åŠ ä½ç½®ç¼–ç åçš„ç‰¹å¾
        """
        batch_size, seq_length, d_model = x.shape
        
        if self.encoding_type == 'learnable':
            # ç”Ÿæˆä½ç½®ç´¢å¼•
            positions = torch.arange(start_pos, start_pos + seq_length, 
                                   device=x.device, dtype=torch.long)
            pos_encoding = self.positional_embedding(positions)  # (seq_length, d_model)
            pos_encoding = pos_encoding.unsqueeze(0).expand(batch_size, -1, -1)
            
        elif self.encoding_type == 'sinusoidal':
            pos_encoding = self.pe[:, start_pos:start_pos + seq_length, :]
            pos_encoding = pos_encoding.expand(batch_size, -1, -1)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + pos_encoding
        return metrics
    
    def eval_on_batch(self, 
                     observations: torch.Tensor,
                     actions: torch.Tensor,
                     return_attention: bool = False,
                     compute_detailed_metrics: bool = True) -> dict:
        """
        åœ¨å•ä¸ªbatchä¸Šè¯„ä¼°æ¨¡å‹
        
        Args:
            observations: (batch_size, seq_length+1, 4, 84, 84) - è§‚å¯Ÿåºåˆ—
            actions: (batch_size, seq_length) - åŠ¨ä½œåºåˆ—
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
            compute_detailed_metrics: æ˜¯å¦è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        
        Returns:
            dict: åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        """
        self.eval()
        
        with torch.no_grad():
            # å‰å‘ä¼ æ’­
            results = self.forward(observations, actions, return_attention=return_attention)
            loss = results['loss']
            predicted = results['predicted']
            target = results['target']
            
            # åŸºç¡€æŒ‡æ ‡
            metrics = {
                'loss': loss.item(),
                'batch_size': observations.shape[0]
            }
            
            if compute_detailed_metrics:
                # MSEè¯¯å·®
                mse_loss = F.mse_loss(predicted, target)
                
                # L1è¯¯å·®
                l1_loss = F.l1_loss(predicted, target)
                
                # ä½™å¼¦ç›¸ä¼¼åº¦
                cos_sim = F.cosine_similarity(predicted, target, dim=-1)
                cos_sim_mean = cos_sim.mean()
                cos_sim_std = cos_sim.std()
                
                # é¢„æµ‹å‡†ç¡®æ€§ï¼ˆåŸºäºä½™å¼¦ç›¸ä¼¼åº¦é˜ˆå€¼ï¼‰
                accuracy_05 = (cos_sim > 0.5).float().mean()
                accuracy_07 = (cos_sim > 0.7).float().mean()
                accuracy_09 = (cos_sim > 0.9).float().mean()
                
                # ç›¸å¯¹è¯¯å·®
                relative_error = ((predicted - target).abs() / (target.abs() + 1e-8)).mean()
                
                # æ–¹å·®è§£é‡Šæ¯”ä¾‹ (ç±»ä¼¼RÂ²)
                target_var = target.var()
                residual_var = (predicted - target).var()
                variance_explained = 1 - (residual_var / (target_var + 1e-8))
                
                metrics.update({
                    'mse_loss': mse_loss.item(),
                    'l1_loss': l1_loss.item(),
                    'cos_similarity_mean': cos_sim_mean.item(),
                    'cos_similarity_std': cos_sim_std.item(),
                    'accuracy_05': accuracy_05.item(),
                    'accuracy_07': accuracy_07.item(),
                    'accuracy_09': accuracy_09.item(),
                    'relative_error': relative_error.item(),
                    'variance_explained': variance_explained.item()
                })
            
            if return_attention:
                metrics['attention_weights'] = results.get('attention_weights')
        
        return metrics
    
    def predict_next_features(self, 
                             observations: torch.Tensor,
                             actions: torch.Tensor,
                             return_attention: bool = False) -> dict:
        """
        é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
        
        Args:
            observations: (batch_size, seq_length+1, 4, 84, 84)
            actions: (batch_size, seq_length)
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
        
        Returns:
            dict: åŒ…å«é¢„æµ‹ç»“æœçš„å­—å…¸
        """
        self.eval()
        
        with torch.no_grad():
            results = self.forward(observations, actions, return_attention=return_attention)
            
            # åªè¿”å›é¢„æµ‹ç»“æœï¼Œä¸è®¡ç®—æŸå¤±
            output = {
                'predicted': results['predicted'],
                'batch_size': observations.shape[0]
            }
            
            if return_attention:
                output['attention_weights'] = results.get('attention_weights')
            
            return output self.dropout(x)

class CustomTransformerBlock(nn.Module):
    """
    è‡ªå®šä¹‰Transformerå—ï¼Œä½¿ç”¨å®˜æ–¹ç»„ä»¶ä½†æ”¯æŒçµæ´»çš„attentioné…ç½®
    """
    
    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        
        self.d_model = d_model
        self.num_heads = num_heads
        
        # ä½¿ç”¨å®˜æ–¹çš„MultiheadAttention - è‡ªæ³¨æ„åŠ›
        self.self_attention = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True  # é‡è¦ï¼šä½¿ç”¨batch_first=True
        )
        
        # ä½¿ç”¨å®˜æ–¹çš„MultiheadAttention - äº¤å‰æ³¨æ„åŠ›
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # ä½¿ç”¨å®˜æ–¹çš„LayerNormå’ŒLinear
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        
        # ä½¿ç”¨å®˜æ–¹çš„FFNç»“æ„
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, 
                img_features: torch.Tensor,
                action_features: torch.Tensor,
                self_attn_mask: Optional[torch.Tensor] = None,
                cross_attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            img_features: (batch_size, seq_length, d_model) - å›¾åƒç‰¹å¾
            action_features: (batch_size, seq_length, d_model) - åŠ¨ä½œç‰¹å¾
            self_attn_mask: è‡ªæ³¨æ„åŠ›æ©ç 
            cross_attn_mask: äº¤å‰æ³¨æ„åŠ›æ©ç 
        
        Returns:
            (batch_size, seq_length, d_model) - è¾“å‡ºç‰¹å¾
        """
        # 1. å›¾åƒç‰¹å¾çš„è‡ªæ³¨æ„åŠ›ï¼ˆå¸¦å› æœæ©ç ï¼‰
        img_attended, _ = self.self_attention(
            query=img_features,
            key=img_features,
            value=img_features,
            attn_mask=self_attn_mask,
            need_weights=False  # æå‡æ€§èƒ½
        )
        img_features = self.norm1(img_features + self.dropout(img_attended))
        
        # 2. äº¤å‰æ³¨æ„åŠ›ï¼šåŠ¨ä½œæŸ¥è¯¢å›¾åƒ
        cross_attended, cross_weights = self.cross_attention(
            query=action_features,      # actionä½œä¸ºquery
            key=img_features,          # imgä½œä¸ºkey
            value=img_features,        # imgä½œä¸ºvalue
            attn_mask=cross_attn_mask,
            need_weights=True  # å¯èƒ½éœ€è¦åˆ†ææ³¨æ„åŠ›æ¨¡å¼
        )
        action_features = self.norm2(action_features + self.dropout(cross_attended))
        
        # 3. FFN
        ffn_output = self.ffn(action_features)
        output = self.norm3(action_features + self.dropout(ffn_output))
        
        return output, cross_weights

class OptimizedDecoderOnlyTransformer(nn.Module):
    """
    ä¼˜åŒ–çš„Decoder-Only Transformerï¼Œä½¿ç”¨å®˜æ–¹ç»„ä»¶ï¼ŒåŒ…å«ä½ç½®ç¼–ç 
    """
    
    def __init__(self, 
                 d_model: int = 512,
                 num_heads: int = 8, 
                 d_ff: int = 2048,
                 num_layers: int = 6,
                 dropout: float = 0.1,
                 seq_length: int = 32,
                 pos_encoding_type: str = 'learnable'):
        super().__init__()
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.seq_length = seq_length
        
        # ä½ç½®ç¼–ç æ¨¡å—
        self.img_pos_encoding = PositionalEncoding(
            d_model=d_model,
            max_seq_length=seq_length + 10,  # ç•™ä¸€äº›ä½™é‡
            encoding_type=pos_encoding_type,
            dropout=dropout
        )
        
        self.action_pos_encoding = PositionalEncoding(
            d_model=d_model,
            max_seq_length=seq_length + 10,
            encoding_type=pos_encoding_type,
            dropout=dropout
        )
        
        # Transformerå±‚
        self.transformer_blocks = nn.ModuleList([
            CustomTransformerBlock(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])
        
        # æœ€ç»ˆå½’ä¸€åŒ–
        self.final_norm = nn.LayerNorm(d_model)
        
        # æ³¨å†Œæ©ç ç¼“å†²åŒºï¼ˆæ”¯æŒåŠ¨æ€ä¿®æ”¹ï¼‰
        self.register_buffer('_causal_mask', torch.empty(0))
        self._init_causal_mask(seq_length)
    
    def _init_causal_mask(self, seq_length: int):
        """åˆå§‹åŒ–å› æœæ©ç """
        # åˆ›å»ºä¸‹ä¸‰è§’æ©ç 
        mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool()
        self.register_buffer('_causal_mask', mask, persistent=False)
    
    def set_causal_mask(self, mask: torch.Tensor):
        """åŠ¨æ€è®¾ç½®å› æœæ©ç """
        self.register_buffer('_causal_mask', mask, persistent=False)
    
    def get_causal_mask(self, seq_length: int) -> torch.Tensor:
        """è·å–é€‚å½“å¤§å°çš„å› æœæ©ç """
        if self._causal_mask.size(0) < seq_length:
            self._init_causal_mask(seq_length)
        return self._causal_mask[:seq_length, :seq_length]
    
    def forward(self, 
                img_features: torch.Tensor, 
                action_features: torch.Tensor,
                return_attention_weights: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, list]]:
        """
        Args:
            img_features: (batch_size, seq_length+1, d_model) - åŸå§‹å›¾åƒç‰¹å¾
            action_features: (batch_size, seq_length, d_model) - åŸå§‹åŠ¨ä½œç‰¹å¾
            return_attention_weights: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
        
        Returns:
            output: (batch_size, seq_length, d_model)
            attention_weights: (å¯é€‰) æ¯å±‚çš„äº¤å‰æ³¨æ„åŠ›æƒé‡
        """
        batch_size, img_seq_len, _ = img_features.shape
        _, action_seq_len, _ = action_features.shape
        
        # å–å‰seq_lengthä¸ªå›¾åƒç‰¹å¾
        img_input = img_features[:, :action_seq_len, :]  # (batch_size, seq_length, d_model)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        img_input = self.img_pos_encoding(img_input, start_pos=0)
        action_features = self.action_pos_encoding(action_features, start_pos=0)
        
        # è·å–å› æœæ©ç 
        causal_mask = self.get_causal_mask(action_seq_len)
        
        # å­˜å‚¨æ³¨æ„åŠ›æƒé‡
        attention_weights = [] if return_attention_weights else None
        
        # é€šè¿‡æ‰€æœ‰Transformerå—
        output = action_features
        for layer in self.transformer_blocks:
            output, cross_weights = layer(
                img_features=img_input,
                action_features=output,
                self_attn_mask=causal_mask,
                cross_attn_mask=None  # äº¤å‰æ³¨æ„åŠ›é€šå¸¸ä¸éœ€è¦æ©ç 
            )
            
            if return_attention_weights:
                attention_weights.append(cross_weights)
        
        # æœ€ç»ˆå½’ä¸€åŒ–
        output = self.final_norm(output)
        
        if return_attention_weights:
            return output, attention_weights
        return output

class SimplePredictor(nn.Module):
    """
    ç®€åŒ–çš„é¢„æµ‹å™¨ï¼Œç›´æ¥ä½¿ç”¨detachç­–ç•¥è·å–ç›®æ ‡
    """
    
    def __init__(self, transformer: OptimizedDecoderOnlyTransformer):
        super().__init__()
        
        self.transformer = transformer
        
        # è¾“å‡ºæŠ•å½±å±‚
        self.output_proj = nn.Sequential(
            nn.Linear(transformer.d_model, transformer.d_model // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(transformer.d_model // 2, transformer.d_model)
        )
    
    def forward(self, 
                img_features: torch.Tensor,
                action_features: torch.Tensor,
                return_attention: bool = False) -> dict:
        """
        Args:
            img_features: (batch_size, seq_length+1, d_model)
            action_features: (batch_size, seq_length, d_model)
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
        
        Returns:
            dict with 'predicted', 'target', and optionally 'attention_weights'
        """
        # å‰å‘é¢„æµ‹
        if return_attention:
            predicted, attention_weights = self.transformer(
                img_features, action_features, return_attention_weights=True
            )
        else:
            predicted = self.transformer(img_features, action_features)
            attention_weights = None
        
        # è¾“å‡ºæŠ•å½±
        predicted = self.output_proj(predicted)
        
        # è·å–ç›®æ ‡ç‰¹å¾ï¼ˆç›´æ¥detachï¼Œæ— æ¢¯åº¦ï¼‰
        target = img_features[:, 1:, :].detach()  # åseq_lengthä¸ªå›¾åƒç‰¹å¾ï¼Œæ— æ¢¯åº¦
        
        result = {
            'predicted': predicted,
            'target': target
        }
        
        if return_attention:
            result['attention_weights'] = attention_weights
        
        return result

class OptimizedEndToEndModel(nn.Module):
    """
    ä¼˜åŒ–çš„ç«¯åˆ°ç«¯æ¨¡å‹ï¼ŒåŒ…å«ä½ç½®ç¼–ç ï¼Œä½¿ç”¨ç®€åŒ–çš„é¢„æµ‹å™¨
    """
    
    def __init__(self, 
                 img_encoder: nn.Module,
                 action_encoder: nn.Module,
                 d_model: int = 512,
                 num_heads: int = 8,
                 d_ff: int = 2048,
                 num_layers: int = 6,
                 dropout: float = 0.1,
                 seq_length: int = 32,
                 loss_type: str = 'combined',
                 pos_encoding_type: str = 'learnable'):
        super().__init__()
        
        self.img_encoder = img_encoder
        self.action_encoder = action_encoder
        self.loss_type = loss_type
        self.seq_length = seq_length
        
        # åˆ›å»ºä¼˜åŒ–çš„transformerï¼ˆåŒ…å«ä½ç½®ç¼–ç ï¼‰
        transformer = OptimizedDecoderOnlyTransformer(
            d_model=d_model,
            num_heads=num_heads,
            d_ff=d_ff,
            num_layers=num_layers,
            dropout=dropout,
            seq_length=seq_length,
            pos_encoding_type=pos_encoding_type
        )
        
        # åˆ›å»ºç®€åŒ–çš„é¢„æµ‹å™¨
        self.predictor = SimplePredictor(transformer=transformer)
    
    def forward(self, 
                observations: torch.Tensor,
                actions: torch.Tensor,
                return_attention: bool = False) -> dict:
        """
        Args:
            observations: (batch_size, seq_length+1, 4, 84, 84)
            actions: (batch_size, seq_length)
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
        
        Returns:
            dict with 'predicted', 'target', 'loss', and optionally 'attention_weights'
        """
        # ç¼–ç 
        img_features = self.img_encoder(observations)  # (batch_size, seq_length+1, d_model)
        action_features = self.action_encoder(actions)  # (batch_size, seq_length, d_model)
        
        # é¢„æµ‹ï¼ˆå†…éƒ¨ä¼šæ·»åŠ ä½ç½®ç¼–ç ï¼‰
        results = self.predictor(
            img_features=img_features,
            action_features=action_features,
            return_attention=return_attention
        )
        
        # è®¡ç®—æŸå¤±
        loss = self._compute_loss(results['predicted'], results['target'])
        results['loss'] = loss
        
        return results
    
    def _compute_loss(self, predicted: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—æŸå¤±"""
        if self.loss_type == 'mse':
            return F.mse_loss(predicted, target)
        elif self.loss_type == 'cosine':
            cos_sim = F.cosine_similarity(predicted, target, dim=-1)
            return (1 - cos_sim).mean()
        elif self.loss_type == 'huber':
            return F.huber_loss(predicted, target)
        elif self.loss_type == 'combined':
            mse_loss = F.mse_loss(predicted, target)
            cos_sim = F.cosine_similarity(predicted, target, dim=-1)
            cos_loss = (1 - cos_sim).mean()
            return mse_loss + 0.1 * cos_loss
        else:
            return F.mse_loss(predicted, target)
    
    def set_causal_mask(self, mask: torch.Tensor):
        """è®¾ç½®è‡ªå®šä¹‰å› æœæ©ç """
        self.predictor.transformer.set_causal_mask(mask)
    
    def train_on_batch(self, 
                      observations: torch.Tensor,
                      actions: torch.Tensor,
                      optimizer: torch.optim.Optimizer,
                      max_grad_norm: Optional[float] = 1.0,
                      return_attention: bool = False) -> dict:
        """
        åœ¨å•ä¸ªbatchä¸Šè®­ç»ƒæ¨¡å‹
        
        Args:
            observations: (batch_size, seq_length+1, 4, 84, 84) - è§‚å¯Ÿåºåˆ—
            actions: (batch_size, seq_length) - åŠ¨ä½œåºåˆ—
            optimizer: ä¼˜åŒ–å™¨
            max_grad_norm: æ¢¯åº¦è£å‰ªçš„æœ€å¤§èŒƒæ•°ï¼ŒNoneè¡¨ç¤ºä¸è£å‰ª
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
        
        Returns:
            dict: åŒ…å«è®­ç»ƒæŒ‡æ ‡çš„å­—å…¸
        """
        self.train()
        
        # å‰å‘ä¼ æ’­
        results = self.forward(observations, actions, return_attention=return_attention)
        loss = results['loss']
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        grad_norm = None
        if max_grad_norm is not None:
            grad_norm = torch.nn.utils.clip_grad_norm_(self.parameters(), max_grad_norm)
        
        # æ›´æ–°å‚æ•°
        optimizer.step()
        
        # è®¡ç®—é¢å¤–çš„æŒ‡æ ‡
        with torch.no_grad():
            predicted = results['predicted']
            target = results['target']
            
            # MSEè¯¯å·®
            mse_loss = F.mse_loss(predicted, target)
            
            # ä½™å¼¦ç›¸ä¼¼åº¦
            cos_sim = F.cosine_similarity(predicted, target, dim=-1).mean()
            
            # L1è¯¯å·®
            l1_loss = F.l1_loss(predicted, target)
            
            # é¢„æµ‹å‡†ç¡®æ€§ï¼ˆåŸºäºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            accuracy = (cos_sim > 0.5).float().mean()
        
        metrics = {
            'loss': loss.item(),
            'mse_loss': mse_loss.item(),
            'l1_loss': l1_loss.item(),
            'cos_similarity': cos_sim.item(),
            'accuracy': accuracy.item(),
            'grad_norm': grad_norm.item() if grad_norm is not None else None,
            'batch_size': observations.shape[0]
        }
        
        if return_attention:
            metrics['attention_weights'] = results.get('attention_weights')
        
        return

# è®­ç»ƒè¾…åŠ©å·¥å…·å‡½æ•°
class TrainingManager:
    """
    è®­ç»ƒç®¡ç†å™¨ï¼Œæä¾›ç»Ÿä¸€çš„è®­ç»ƒæ¥å£
    """
    
    def __init__(self, 
                 model: OptimizedEndToEndModel,
                 optimizer: torch.optim.Optimizer,
                 scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
                 max_grad_norm: float = 1.0,
                 device: str = 'cuda'):
        
        self.model = model.to(device)
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.max_grad_norm = max_grad_norm
        self.device = device
        
        # è®­ç»ƒå†å²
        self.train_history = []
        self.eval_history = []
        self.step_count = 0
    
    def train_epoch(self, 
                   dataloader,
                   log_interval: int = 100,
                   return_attention_freq: int = 0) -> dict:
        """
        è®­ç»ƒä¸€ä¸ªepoch
        
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
            log_interval: æ—¥å¿—æ‰“å°é—´éš”
            return_attention_freq: è¿”å›æ³¨æ„åŠ›æƒé‡çš„é¢‘ç‡ï¼ˆ0è¡¨ç¤ºä¸è¿”å›ï¼‰
        
        Returns:
            dict: epochè®­ç»ƒç»Ÿè®¡
        """
        self.model.train()
        epoch_metrics = {
            'loss': 0.0,
            'mse_loss': 0.0,
            'l1_loss': 0.0,
            'cos_similarity': 0.0,
            'accuracy': 0.0,
            'grad_norm': 0.0,
            'num_batches': 0
        }
        
        attention_samples = []
        
        for batch_idx, (observations, actions) in enumerate(dataloader):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            observations = observations.to(self.device)
            actions = actions.to(self.device)
            
            # æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
            return_attention = (return_attention_freq > 0 and 
                              batch_idx % return_attention_freq == 0)
            
            # è®­ç»ƒä¸€ä¸ªbatch
            metrics = self.model.train_on_batch(
                observations=observations,
                actions=actions,
                optimizer=self.optimizer,
                max_grad_norm=self.max_grad_norm,
                return_attention=return_attention
            )
            
            # ç´¯ç§¯æŒ‡æ ‡
            for key in epoch_metrics:
                if key in metrics and metrics[key] is not None:
                    epoch_metrics[key] += metrics[key]
            epoch_metrics['num_batches'] += 1
            self.step_count += 1
            
            # ä¿å­˜æ³¨æ„åŠ›æƒé‡æ ·æœ¬
            if return_attention and 'attention_weights' in metrics:
                attention_samples.append({
                    'batch_idx': batch_idx,
                    'step': self.step_count,
                    'attention_weights': metrics['attention_weights']
                })
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler is not None:
                self.scheduler.step()
            
            # æ‰“å°æ—¥å¿—
            if batch_idx % log_interval == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                print(f"Batch {batch_idx}/{len(dataloader)}: "
                      f"Loss={metrics['loss']:.6f}, "
                      f"CosSim={metrics['cos_similarity']:.4f}, "
                      f"LR={current_lr:.2e}")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        for key in epoch_metrics:
            if key != 'num_batches':
                epoch_metrics[key] /= epoch_metrics['num_batches']
        
        epoch_metrics['attention_samples'] = attention_samples
        self.train_history.append(epoch_metrics)
        
        return epoch_metrics
    
    def evaluate(self, 
                dataloader,
                return_attention_freq: int = 0) -> dict:
        """
        è¯„ä¼°æ¨¡å‹
        
        Args:
            dataloader: éªŒè¯æ•°æ®åŠ è½½å™¨
            return_attention_freq: è¿”å›æ³¨æ„åŠ›æƒé‡çš„é¢‘ç‡
        
        Returns:
            dict: è¯„ä¼°ç»Ÿè®¡
        """
        self.model.eval()
        eval_metrics = {
            'loss': 0.0,
            'mse_loss': 0.0,
            'l1_loss': 0.0,
            'cos_similarity_mean': 0.0,
            'cos_similarity_std': 0.0,
            'accuracy_05': 0.0,
            'accuracy_07': 0.0,
            'accuracy_09': 0.0,
            'relative_error': 0.0,
            'variance_explained': 0.0,
            'num_batches': 0
        }
        
        attention_samples = []
        
        with torch.no_grad():
            for batch_idx, (observations, actions) in enumerate(dataloader):
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                observations = observations.to(self.device)
                actions = actions.to(self.device)
                
                # æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
                return_attention = (return_attention_freq > 0 and 
                                  batch_idx % return_attention_freq == 0)
                
                # è¯„ä¼°ä¸€ä¸ªbatch
                metrics = self.model.eval_on_batch(
                    observations=observations,
                    actions=actions,
                    return_attention=return_attention,
                    compute_detailed_metrics=True
                )
                
                # ç´¯ç§¯æŒ‡æ ‡
                for key in eval_metrics:
                    if key in metrics and metrics[key] is not None:
                        eval_metrics[key] += metrics[key]
                eval_metrics['num_batches'] += 1
                
                # ä¿å­˜æ³¨æ„åŠ›æƒé‡æ ·æœ¬
                if return_attention and 'attention_weights' in metrics:
                    attention_samples.append({
                        'batch_idx': batch_idx,
                        'attention_weights': metrics['attention_weights']
                    })
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        for key in eval_metrics:
            if key != 'num_batches':
                eval_metrics[key] /= eval_metrics['num_batches']
        
        eval_metrics['attention_samples'] = attention_samples
        self.eval_history.append(eval_metrics)
        
        return eval_metrics
    
    def save_checkpoint(self, filepath: str, epoch: int, best_metric: float = None):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'step_count': self.step_count,
            'train_history': self.train_history,
            'eval_history': self.eval_history,
            'best_metric': best_metric
        }
        
        if self.scheduler is not None:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        torch.save(checkpoint, filepath)
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_checkpoint(self, filepath: str) -> dict:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.step_count = checkpoint.get('step_count', 0)
        self.train_history = checkpoint.get('train_history', [])
        self.eval_history = checkpoint.get('eval_history', [])
        
        if self.scheduler is not None and 'scheduler_state_dict' in checkpoint:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        print(f"æ£€æŸ¥ç‚¹å·²ä» {filepath} åŠ è½½")
        return checkpoint

def create_training_manager(model: OptimizedEndToEndModel,
                           learning_rate: float = 1e-4,
                           weight_decay: float = 1e-5,
                           scheduler_type: str = 'cosine',
                           max_grad_norm: float = 1.0,
                           device: str = 'cuda',
                           **scheduler_kwargs) -> TrainingManager:
    """
    åˆ›å»ºè®­ç»ƒç®¡ç†å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model: æ¨¡å‹
        learning_rate: å­¦ä¹ ç‡
        weight_decay: æƒé‡è¡°å‡
        scheduler_type: è°ƒåº¦å™¨ç±»å‹ ('cosine', 'step', 'plateau', None)
        max_grad_norm: æ¢¯åº¦è£å‰ªèŒƒæ•°
        device: è®¾å¤‡
        **scheduler_kwargs: è°ƒåº¦å™¨å‚æ•°
    
    Returns:
        TrainingManager: è®­ç»ƒç®¡ç†å™¨
    """
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        weight_decay=weight_decay
    )
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = None
    if scheduler_type == 'cosine':
        T_max = scheduler_kwargs.get('T_max', 1000)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=T_max
        )
    elif scheduler_type == 'step':
        step_size = scheduler_kwargs.get('step_size', 100)
        gamma = scheduler_kwargs.get('gamma', 0.1)
        scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer, step_size=step_size, gamma=gamma
        )
    elif scheduler_type == 'plateau':
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', patience=10, factor=0.5
        )
    
    return TrainingManager(
        model=model,
        optimizer=optimizer,
        scheduler=scheduler,
        max_grad_norm=max_grad_norm,
        device=device
    )
def create_optimized_model(img_encoder: nn.Module,
                          action_encoder: nn.Module,
                          config: str = "standard",
                          pos_encoding_type: str = "learnable",
                          **kwargs) -> OptimizedEndToEndModel:
    """
    åˆ›å»ºä¼˜åŒ–çš„æ¨¡å‹
    
    Args:
        img_encoder: å›¾åƒç¼–ç å™¨
        action_encoder: åŠ¨ä½œç¼–ç å™¨
        config: é…ç½®åç§° ("lightweight", "standard", "large")
        pos_encoding_type: ä½ç½®ç¼–ç ç±»å‹ ("learnable", "sinusoidal")
        **kwargs: å…¶ä»–å‚æ•°
    """
    configs = {
        "lightweight": {
            "d_model": 256, "num_heads": 8, "d_ff": 1024,
            "num_layers": 4, "dropout": 0.1
        },
        "standard": {
            "d_model": 512, "num_heads": 8, "d_ff": 2048,
            "num_layers": 6, "dropout": 0.1
        },
        "large": {
            "d_model": 768, "num_heads": 12, "d_ff": 3072,
            "num_layers": 8, "dropout": 0.1
        }
    }
    
    config_params = configs.get(config, configs["standard"])
    config_params.update(kwargs)
    config_params['pos_encoding_type'] = pos_encoding_type
    
    model = OptimizedEndToEndModel(
        img_encoder=img_encoder,
        action_encoder=action_encoder,
        **config_params
    )
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"åˆ›å»ºç®€åŒ–æ¨¡å‹ ({config}):")
    print(f"  å‚æ•°é‡: {total_params:,}")
    print(f"  ä½ç½®ç¼–ç : {pos_encoding_type}")
    print(f"  ç›®æ ‡ç­–ç•¥: detach (æ— æ¢¯åº¦)")
    print(f"  ä½¿ç”¨å®˜æ–¹MultiheadAttention: âœ…")
    print(f"  æ”¯æŒè‡ªå®šä¹‰æ©ç : âœ…")
    
    return model

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("=== ç®€åŒ–çš„Transformeræ¨¡å‹æµ‹è¯•ï¼ˆåŒ…å«è®­ç»ƒåŠŸèƒ½ï¼‰===")
    
    # åˆ›å»ºæ¨¡æ‹Ÿç¼–ç å™¨
    class MockImgEncoder(nn.Module):
        def __init__(self, d_model=512):
            super().__init__()
            self.conv = nn.Conv2d(4, 64, 3, 2, 1)
            self.pool = nn.AdaptiveAvgPool2d(1)
            self.fc = nn.Linear(64, d_model)
        
        def forward(self, x):
            b, s = x.shape[:2]
            x = x.view(b*s, 4, 84, 84)
            x = self.pool(self.conv(x)).view(b*s, 64)
            x = self.fc(x)
            return x.view(b, s, -1)
    
    class MockActionEncoder(nn.Module):
        def __init__(self, d_model=512):
            super().__init__()
            self.emb = nn.Embedding(18, d_model)
        
        def forward(self, x):
            return self.emb(x)
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®åŠ è½½å™¨
    class MockDataset:
        def __init__(self, num_samples=100, seq_length=32):
            self.num_samples = num_samples
            self.seq_length = seq_length
        
        def __len__(self):
            return self.num_samples
        
        def __getitem__(self, idx):
            observations = torch.randn(self.seq_length + 1, 4, 84, 84)
            actions = torch.randint(0, 18, (self.seq_length,))
            return observations, actions
    
    def create_dataloader(dataset, batch_size=4, shuffle=True):
        return torch.utils.data.DataLoader(
            dataset, batch_size=batch_size, shuffle=shuffle
        )
    
    # åˆ›å»ºç¼–ç å™¨å’Œæ¨¡å‹
    img_encoder = MockImgEncoder(512)
    action_encoder = MockActionEncoder(512)
    
    print(f"\n=== æµ‹è¯•åŸºç¡€åŠŸèƒ½ ===")
    
    # åˆ›å»ºç®€åŒ–æ¨¡å‹
    model = create_optimized_model(
        img_encoder=img_encoder,
        action_encoder=action_encoder,
        config="standard",
        seq_length=32,
        pos_encoding_type="learnable"
    )
    
    # æµ‹è¯•æ•°æ®
    batch_size = 4
    seq_length = 32
    observations = torch.randn(batch_size, seq_length + 1, 4, 84, 84)
    actions = torch.randint(0, 18, (batch_size, seq_length))
    
    print(f"è¾“å…¥æ•°æ®:")
    print(f"  è§‚å¯Ÿ: {observations.shape}")
    print(f"  åŠ¨ä½œ: {actions.shape}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        results = model(observations, actions, return_attention=True)
    
    print(f"è¾“å‡ºç»“æœ:")
    print(f"  é¢„æµ‹: {results['predicted'].shape}")
    print(f"  ç›®æ ‡: {results['target'].shape}")
    print(f"  ç›®æ ‡æ¢¯åº¦ä¿¡æ¯: {results['target'].requires_grad}")
    print(f"  æŸå¤±: {results['loss'].item():.6f}")
    
    print(f"\n=== æµ‹è¯•è®­ç»ƒåŠŸèƒ½ ===")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
    
    # æµ‹è¯•å•batchè®­ç»ƒ
    print("æµ‹è¯•train_on_batch:")
    train_metrics = model.train_on_batch(
        observations=observations,
        actions=actions,
        optimizer=optimizer,
        max_grad_norm=1.0,
        return_attention=False
    )
    
    print(f"  è®­ç»ƒæŸå¤±: {train_metrics['loss']:.6f}")
    print(f"  MSEæŸå¤±: {train_metrics['mse_loss']:.6f}")
    print(f"  ä½™å¼¦ç›¸ä¼¼åº¦: {train_metrics['cos_similarity']:.4f}")
    print(f"  æ¢¯åº¦èŒƒæ•°: {train_metrics['grad_norm']:.6f}")
    print(f"  å‡†ç¡®ç‡: {train_metrics['accuracy']:.4f}")
    
    # æµ‹è¯•å•batchè¯„ä¼°
    print("\næµ‹è¯•eval_on_batch:")
    eval_metrics = model.eval_on_batch(
        observations=observations,
        actions=actions,
        return_attention=False,
        compute_detailed_metrics=True
    )
    
    print(f"  è¯„ä¼°æŸå¤±: {eval_metrics['loss']:.6f}")
    print(f"  MSEæŸå¤±: {eval_metrics['mse_loss']:.6f}")
    print(f"  ä½™å¼¦ç›¸ä¼¼åº¦(å‡å€¼): {eval_metrics['cos_similarity_mean']:.4f}")
    print(f"  ä½™å¼¦ç›¸ä¼¼åº¦(æ ‡å‡†å·®): {eval_metrics['cos_similarity_std']:.4f}")
    print(f"  å‡†ç¡®ç‡(>0.5): {eval_metrics['accuracy_05']:.4f}")
    print(f"  å‡†ç¡®ç‡(>0.7): {eval_metrics['accuracy_07']:.4f}")
    print(f"  å‡†ç¡®ç‡(>0.9): {eval_metrics['accuracy_09']:.4f}")
    print(f"  ç›¸å¯¹è¯¯å·®: {eval_metrics['relative_error']:.6f}")
    print(f"  æ–¹å·®è§£é‡Šæ¯”ä¾‹: {eval_metrics['variance_explained']:.4f}")
    
    # æµ‹è¯•é¢„æµ‹åŠŸèƒ½
    print("\næµ‹è¯•predict_next_features:")
    pred_results = model.predict_next_features(
        observations=observations,
        actions=actions,
        return_attention=True
    )
    
    print(f"  é¢„æµ‹å½¢çŠ¶: {pred_results['predicted'].shape}")
    print(f"  æ³¨æ„åŠ›æƒé‡å±‚æ•°: {len(pred_results['attention_weights'])}")
    
    print(f"\n=== æµ‹è¯•è®­ç»ƒç®¡ç†å™¨ ===")
    
    # åˆ›å»ºè®­ç»ƒç®¡ç†å™¨
    trainer = create_training_manager(
        model=model,
        learning_rate=1e-4,
        weight_decay=1e-5,
        scheduler_type='cosine',
        max_grad_norm=1.0,
        device='cpu',  # ä½¿ç”¨CPUè¿›è¡Œæµ‹è¯•
        T_max=100
    )
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    train_dataset = MockDataset(num_samples=20, seq_length=32)
    val_dataset = MockDataset(num_samples=10, seq_length=32)
    
    train_loader = create_dataloader(train_dataset, batch_size=4, shuffle=True)
    val_loader = create_dataloader(val_dataset, batch_size=4, shuffle=False)
    
    print("è®­ç»ƒæ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
    print(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"  éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    
    # æµ‹è¯•ä¸€ä¸ªepochçš„è®­ç»ƒ
    print("\nè®­ç»ƒä¸€ä¸ªepoch:")
    epoch_metrics = trainer.train_epoch(
        dataloader=train_loader,
        log_interval=2,
        return_attention_freq=0
    )
    
    print(f"Epochè®­ç»ƒç»“æœ:")
    print(f"  å¹³å‡æŸå¤±: {epoch_metrics['loss']:.6f}")
    print(f"  å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦: {epoch_metrics['cos_similarity']:.4f}")
    print(f"  å¹³å‡æ¢¯åº¦èŒƒæ•°: {epoch_metrics['grad_norm']:.6f}")
    
    # æµ‹è¯•è¯„ä¼°
    print("\nè¯„ä¼°æ¨¡å‹:")
    eval_results = trainer.evaluate(
        dataloader=val_loader,
        return_attention_freq=0
    )
    
    print(f"è¯„ä¼°ç»“æœ:")
    print(f"  è¯„ä¼°æŸå¤±: {eval_results['loss']:.6f}")
    print(f"  ä½™å¼¦ç›¸ä¼¼åº¦: {eval_results['cos_similarity_mean']:.4f}")
    print(f"  å‡†ç¡®ç‡(>0.5): {eval_results['accuracy_05']:.4f}")
    print(f"  æ–¹å·®è§£é‡Šæ¯”ä¾‹: {eval_results['variance_explained']:.4f}")
    
    # æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜å’ŒåŠ è½½
    print("\næµ‹è¯•æ£€æŸ¥ç‚¹åŠŸèƒ½:")
    import tempfile
    import os
    
    with tempfile.TemporaryDirectory() as temp_dir:
        checkpoint_path = os.path.join(temp_dir, 'test_checkpoint.pth')
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        trainer.save_checkpoint(
            filepath=checkpoint_path,
            epoch=1,
            best_metric=eval_results['loss']
        )
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint_info = trainer.load_checkpoint(checkpoint_path)
        print(f"  æ£€æŸ¥ç‚¹åŒ…å«epoch: {checkpoint_info['epoch']}")
        print(f"  æœ€ä½³æŒ‡æ ‡: {checkpoint_info['best_metric']:.6f}")
    
    print(f"\n=== æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")
    
    # æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½
    configs = ['lightweight', 'standard']
    
    for config in configs:
        print(f"\næµ‹è¯• {config} é…ç½®:")
        
        test_model = create_optimized_model(
            img_encoder=img_encoder,
            action_encoder=action_encoder,
            config=config,
            seq_length=32,
            pos_encoding_type="learnable"
        )
        
        # æµ‹è¯•å‰å‘ä¼ æ’­æ—¶é—´
        import time
        
        test_model.eval()
        with torch.no_grad():
            start_time = time.time()
            for _ in range(10):
                _ = test_model(observations, actions)
            end_time = time.time()
        
        avg_time = (end_time - start_time) / 10
        print(f"  å¹³å‡å‰å‘ä¼ æ’­æ—¶é—´: {avg_time*1000:.2f} ms")
    
    print(f"\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    print(f"ğŸ“ˆ è®­ç»ƒåŠŸèƒ½å®Œæ•´ï¼štrain_on_batch, eval_on_batch, TrainingManager")
    print(f"ğŸ’¾ æ”¯æŒæ£€æŸ¥ç‚¹ä¿å­˜å’ŒåŠ è½½")
    print(f"ğŸ“Š æä¾›è¯¦ç»†çš„è®­ç»ƒå’Œè¯„ä¼°æŒ‡æ ‡")
    print(f"ğŸ”§ æ”¯æŒå¤šç§ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨")